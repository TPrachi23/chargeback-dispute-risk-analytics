{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmv4xmBu_J7C",
        "outputId": "11bee753-7b5c-4f05-9746-7188544b8b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROOT: /content\n",
            "DATA_DIR: /content/data\n",
            "OUT_DIR: /content/outputs\n",
            "\n",
            "Saved raw data:\n",
            " - data/merchants.csv\n",
            " - data/transactions.csv\n",
            " - data/disputes.csv\n",
            "\n",
            "Counts => merchants=250 txns=120,000 disputes=2,038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-474193161.py:355: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  win_ev = disputes.groupby(\"evidence_bucket\", as_index=False).agg(win_rate=(\"outcome\", lambda x: (x == \"WIN\").mean()),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved outputs:\n",
            " - outputs/merchant_month_risk_metrics.csv\n",
            " - outputs/dispute_reason_overall.csv\n",
            " - outputs/dispute_reason_by_tier.csv\n",
            " - outputs/win_loss_drivers_slices.csv\n",
            " - outputs/cohort_risk_summary.csv\n",
            " - outputs/overall_chargeback_rate_trend.png\n",
            " - outputs/dispute_volume_by_reason.png\n",
            " - outputs/win_rate_by_evidence_bucket.png\n",
            " - outputs/cohort_chargeback_rate_table.png\n",
            "\n",
            "Top 10 reason codes:\n",
            "     reason_code  disputes  dispute_amount  win_rate\n",
            "           FRAUD       588        29300.80  0.340136\n",
            "NOT_AS_DESCRIBED       375        19415.17  0.312000\n",
            "    NOT_RECEIVED       371        18277.29  0.385445\n",
            "         NO_AUTH       325        14096.75  0.384615\n",
            "       DUPLICATE       149         7861.15  0.543624\n",
            "PROCESSING_ERROR       136         6914.71  0.411765\n",
            "       CANCELLED        94         5908.19  0.361702\n",
            "\n",
            "Cohort summary (sample):\n",
            "risk_tier tenure_cohort  merchants  txn_count  disputes  txn_amount  net_loss_usd  chargeback_rate  net_loss_rate\n",
            "     High          0-5m          4       3014     122.0   143033.07       6577.95         0.040478       0.045989\n",
            "     High        12-23m          6       3621     107.0   175817.38       5466.25         0.029550       0.031090\n",
            "     High        24-35m          9       5865     135.0   281222.49       5929.28         0.023018       0.021084\n",
            "     High          36m+         11       5761     147.0   278033.65       6326.99         0.025516       0.022756\n",
            "     High         6-11m          6       3170     126.0   155591.15       7200.11         0.039748       0.046276\n",
            "      Low          0-5m          4       1481      26.0    71922.93       1246.39         0.017556       0.017330\n",
            "      Low        12-23m         30      12611     164.0   616689.68       7625.22         0.013005       0.012365\n",
            "      Low        24-35m         30      13137     118.0   641662.29       5414.50         0.008982       0.008438\n",
            "      Low          36m+         59      29015     323.0  1403259.38      10074.82         0.011132       0.007180\n",
            "      Low         6-11m         15       5219     102.0   250048.10       4168.38         0.019544       0.016670\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# 0) Repro + folders\n",
        "# ---------------------------\n",
        "SEED = 42\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "ROOT = os.getcwd()  # Colab => /content\n",
        "DATA_DIR = os.path.join(ROOT, \"data\")\n",
        "OUT_DIR = os.path.join(ROOT, \"outputs\")\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"ROOT:\", ROOT)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Synthetic Data Generator\n",
        "# ---------------------------\n",
        "def make_merchants(n_merchants=250):\n",
        "    merchant_id = [f\"M{str(i).zfill(4)}\" for i in range(1, n_merchants + 1)]\n",
        "\n",
        "    tenure_months = rng.integers(1, 60, size=n_merchants)  # 1 to 59\n",
        "    risk_tier = rng.choice([\"Low\", \"Medium\", \"High\"], size=n_merchants, p=[0.55, 0.30, 0.15])\n",
        "\n",
        "    # baseline monthly volume correlated with risk tier slightly\n",
        "    base_volume = rng.lognormal(mean=10.2, sigma=0.55, size=n_merchants)  # ~$27k median-ish\n",
        "    tier_mult = np.where(risk_tier == \"High\", 1.15, np.where(risk_tier == \"Medium\", 1.00, 0.90))\n",
        "    monthly_volume_usd = np.round(base_volume * tier_mult, 2)\n",
        "\n",
        "    # refund rate: higher in some risk tiers and lower tenure (slightly)\n",
        "    refund_base = rng.beta(a=2.2, b=18.0, size=n_merchants)  # ~10% typical\n",
        "    refund_rate = np.clip(refund_base + (risk_tier == \"High\") * 0.03 - (tenure_months / 120) * 0.01, 0.0, 0.35)\n",
        "\n",
        "    merchant_df = pd.DataFrame({\n",
        "        \"merchant_id\": merchant_id,\n",
        "        \"risk_tier\": risk_tier,\n",
        "        \"tenure_months\": tenure_months,\n",
        "        \"monthly_volume_usd\": monthly_volume_usd,\n",
        "        \"refund_rate\": np.round(refund_rate, 4)\n",
        "    })\n",
        "    return merchant_df\n",
        "\n",
        "\n",
        "def make_transactions(merchant_df, n_txn=120_000, start=\"2024-01-01\", end=\"2025-12-31\"):\n",
        "    # date range\n",
        "    start_dt = pd.Timestamp(start)\n",
        "    end_dt = pd.Timestamp(end)\n",
        "    days = (end_dt - start_dt).days + 1\n",
        "    txn_dates = start_dt + pd.to_timedelta(rng.integers(0, days, size=n_txn), unit=\"D\")\n",
        "\n",
        "    merchants = merchant_df[\"merchant_id\"].to_numpy()\n",
        "    # Weighted by monthly volume (bigger merchants = more transactions)\n",
        "    w = merchant_df[\"monthly_volume_usd\"].to_numpy()\n",
        "    w = w / w.sum()\n",
        "    txn_merchant = rng.choice(merchants, size=n_txn, p=w)\n",
        "\n",
        "    channel = rng.choice([\"CNP\", \"CP\"], size=n_txn, p=[0.78, 0.22])\n",
        "    country = rng.choice([\"US\", \"CA\", \"UK\", \"IN\", \"AU\", \"BR\", \"MX\", \"DE\"], size=n_txn,\n",
        "                         p=[0.55, 0.07, 0.07, 0.08, 0.05, 0.06, 0.06, 0.06])\n",
        "\n",
        "    # Amounts: lognormal but capped\n",
        "    amt = rng.lognormal(mean=3.6, sigma=0.75, size=n_txn)  # ~ $36 median-ish\n",
        "    amt = np.clip(amt, 2, 800)\n",
        "    amount = np.round(amt, 2)\n",
        "\n",
        "    txn_df = pd.DataFrame({\n",
        "        \"txn_id\": [f\"T{str(i).zfill(7)}\" for i in range(1, n_txn + 1)],\n",
        "        \"txn_date\": pd.to_datetime(txn_dates),\n",
        "        \"merchant_id\": txn_merchant,\n",
        "        \"channel\": channel,\n",
        "        \"country\": country,\n",
        "        \"amount_usd\": amount\n",
        "    })\n",
        "\n",
        "    return txn_df\n",
        "\n",
        "\n",
        "def make_disputes(txn_df, merchant_df, dispute_rate_base=0.012):\n",
        "    # Merge merchant features into txn-level for rate modeling\n",
        "    m = merchant_df.set_index(\"merchant_id\")\n",
        "    df = txn_df.join(m, on=\"merchant_id\")\n",
        "\n",
        "    # Probability model (simple but realistic directional effects):\n",
        "    # - higher for High risk tier\n",
        "    # - higher for CNP\n",
        "    # - higher for higher amounts (small effect)\n",
        "    # - higher if refund_rate high (friendly fraud / operational patterns)\n",
        "    tier_adj = np.where(df[\"risk_tier\"].values == \"High\", 0.012,\n",
        "                np.where(df[\"risk_tier\"].values == \"Medium\", 0.004, -0.002))\n",
        "    channel_adj = np.where(df[\"channel\"].values == \"CNP\", 0.006, -0.003)\n",
        "    amt_adj = np.clip((df[\"amount_usd\"].values - 50) / 5000, -0.002, 0.004)\n",
        "    refund_adj = np.clip((df[\"refund_rate\"].values - 0.10) * 0.08, -0.004, 0.010)\n",
        "    tenure_adj = np.clip((12 - df[\"tenure_months\"].values) / 600, -0.004, 0.006)  # newer => slightly higher\n",
        "\n",
        "    p = dispute_rate_base + tier_adj + channel_adj + amt_adj + refund_adj + tenure_adj\n",
        "    p = np.clip(p, 0.001, 0.09)\n",
        "\n",
        "    is_dispute = rng.random(len(df)) < p\n",
        "    dispute_txns = df.loc[is_dispute].copy()\n",
        "\n",
        "    # Reason codes distribution changes by channel/tier a bit\n",
        "    reason_pool = [\"FRAUD\", \"NO_AUTH\", \"NOT_RECEIVED\", \"NOT_AS_DESCRIBED\", \"DUPLICATE\", \"CANCELLED\", \"PROCESSING_ERROR\"]\n",
        "    # Base probabilities\n",
        "    base_probs = np.array([0.23, 0.17, 0.18, 0.20, 0.07, 0.07, 0.08])\n",
        "\n",
        "    # Adjust probabilities for CNP and High tier\n",
        "    adj = []\n",
        "    for _, r in dispute_txns.iterrows():\n",
        "        probs = base_probs.copy()\n",
        "        if r[\"channel\"] == \"CNP\":\n",
        "            probs[0] += 0.07  # FRAUD\n",
        "            probs[2] += 0.03  # NOT_RECEIVED\n",
        "            probs[5] -= 0.02  # CANCELLED\n",
        "        if r[\"risk_tier\"] == \"High\":\n",
        "            probs[0] += 0.04\n",
        "            probs[1] += 0.02\n",
        "            probs[6] -= 0.02\n",
        "        probs = np.clip(probs, 0.01, None)\n",
        "        probs = probs / probs.sum()\n",
        "        adj.append(probs)\n",
        "\n",
        "    adj = np.vstack(adj)\n",
        "    reason_idx = [rng.choice(len(reason_pool), p=adj[i]) for i in range(adj.shape[0])]\n",
        "    dispute_txns[\"reason_code\"] = [reason_pool[i] for i in reason_idx]\n",
        "\n",
        "    # Evidence quality score (0-100): better for older merchants + lower risk\n",
        "    eq = (\n",
        "        55\n",
        "        + (dispute_txns[\"tenure_months\"].values / 60) * 25\n",
        "        - (dispute_txns[\"risk_tier\"].values == \"High\") * 10\n",
        "        + rng.normal(0, 12, size=len(dispute_txns))\n",
        "    )\n",
        "    dispute_txns[\"evidence_quality\"] = np.clip(eq, 0, 100).round(0).astype(int)\n",
        "\n",
        "    # Response time days: faster for better ops; slower for high risk / new merchants\n",
        "    rt = (\n",
        "        7\n",
        "        - (dispute_txns[\"tenure_months\"].values / 60) * 2\n",
        "        + (dispute_txns[\"risk_tier\"].values == \"High\") * 2\n",
        "        + rng.normal(0, 2.5, size=len(dispute_txns))\n",
        "    )\n",
        "    dispute_txns[\"response_time_days\"] = np.clip(rt, 0, 30).round(0).astype(int)\n",
        "\n",
        "    # Win probability model:\n",
        "    # - Higher evidence quality\n",
        "    # - Lower response time\n",
        "    # - Certain reason codes easier/harder (e.g., processing errors easier to win)\n",
        "    reason_win_adj = dispute_txns[\"reason_code\"].map({\n",
        "        \"PROCESSING_ERROR\": 0.18,\n",
        "        \"DUPLICATE\": 0.12,\n",
        "        \"CANCELLED\": 0.08,\n",
        "        \"NOT_AS_DESCRIBED\": -0.05,\n",
        "        \"NOT_RECEIVED\": -0.03,\n",
        "        \"NO_AUTH\": -0.08,\n",
        "        \"FRAUD\": -0.12\n",
        "    }).values\n",
        "\n",
        "    z = (\n",
        "        -0.3\n",
        "        + (dispute_txns[\"evidence_quality\"].values - 60) / 25\n",
        "        - (dispute_txns[\"response_time_days\"].values) / 18\n",
        "        + reason_win_adj\n",
        "        - (dispute_txns[\"risk_tier\"].values == \"High\") * 0.10\n",
        "    )\n",
        "    win_prob = 1 / (1 + np.exp(-z))\n",
        "    win_prob = np.clip(win_prob, 0.02, 0.95)\n",
        "\n",
        "    outcome = np.where(rng.random(len(dispute_txns)) < win_prob, \"WIN\", \"LOSS\")\n",
        "    dispute_txns[\"outcome\"] = outcome\n",
        "\n",
        "    # Dispute amounts: usually full amount, sometimes partial\n",
        "    partial = rng.random(len(dispute_txns)) < 0.12\n",
        "    dispute_amt = np.where(partial, dispute_txns[\"amount_usd\"].values * rng.uniform(0.3, 0.9, size=len(dispute_txns)),\n",
        "                           dispute_txns[\"amount_usd\"].values)\n",
        "    dispute_txns[\"dispute_amount_usd\"] = np.round(dispute_amt, 2)\n",
        "\n",
        "    # Fees: simplified fixed + % (only on loss)\n",
        "    fee = 15 + 0.01 * dispute_txns[\"dispute_amount_usd\"].values\n",
        "    dispute_txns[\"cb_fee_usd\"] = np.round(fee, 2)\n",
        "\n",
        "    # Final disputes table\n",
        "    disputes = dispute_txns[[\n",
        "        \"txn_id\", \"txn_date\", \"merchant_id\", \"channel\", \"country\",\n",
        "        \"amount_usd\", \"reason_code\", \"dispute_amount_usd\",\n",
        "        \"evidence_quality\", \"response_time_days\", \"outcome\", \"cb_fee_usd\",\n",
        "        \"risk_tier\", \"tenure_months\", \"monthly_volume_usd\", \"refund_rate\"\n",
        "    ]].copy()\n",
        "\n",
        "    disputes.insert(0, \"dispute_id\", [f\"D{str(i).zfill(7)}\" for i in range(1, len(disputes) + 1)])\n",
        "    disputes.rename(columns={\"txn_date\": \"dispute_open_date\"}, inplace=True)\n",
        "\n",
        "    return disputes\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Build data\n",
        "# ---------------------------\n",
        "merchants = make_merchants(n_merchants=250)\n",
        "transactions = make_transactions(merchants, n_txn=120_000, start=\"2024-01-01\", end=\"2025-12-31\")\n",
        "disputes = make_disputes(transactions, merchants, dispute_rate_base=0.012)\n",
        "\n",
        "# Save raw data\n",
        "merchants.to_csv(os.path.join(DATA_DIR, \"merchants.csv\"), index=False)\n",
        "transactions.to_csv(os.path.join(DATA_DIR, \"transactions.csv\"), index=False)\n",
        "disputes.to_csv(os.path.join(DATA_DIR, \"disputes.csv\"), index=False)\n",
        "\n",
        "print(\"\\nSaved raw data:\")\n",
        "print(\" - data/merchants.csv\")\n",
        "print(\" - data/transactions.csv\")\n",
        "print(\" - data/disputes.csv\")\n",
        "print(f\"\\nCounts => merchants={len(merchants):,} txns={len(transactions):,} disputes={len(disputes):,}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Analytics\n",
        "# ---------------------------\n",
        "\n",
        "# Helper: month bucket\n",
        "transactions[\"month\"] = transactions[\"txn_date\"].dt.to_period(\"M\").astype(str)\n",
        "disputes[\"month\"] = pd.to_datetime(disputes[\"dispute_open_date\"]).dt.to_period(\"M\").astype(str)\n",
        "\n",
        "# 3.1 Chargeback rate & loss analysis (merchant-month)\n",
        "txn_mm = transactions.groupby([\"merchant_id\", \"month\"], as_index=False).agg(\n",
        "    txn_count=(\"txn_id\", \"count\"),\n",
        "    txn_amount=(\"amount_usd\", \"sum\")\n",
        ")\n",
        "\n",
        "disp_mm = disputes.groupby([\"merchant_id\", \"month\"], as_index=False).agg(\n",
        "    dispute_count=(\"dispute_id\", \"count\"),\n",
        "    dispute_amount=(\"dispute_amount_usd\", \"sum\"),\n",
        "    loss_amount=(\"dispute_amount_usd\", lambda s: s.sum()),  # placeholder; will adjust below\n",
        "    loss_fees=(\"cb_fee_usd\", \"sum\")\n",
        ")\n",
        "\n",
        "# True losses only when outcome == LOSS\n",
        "loss_mm = disputes[disputes[\"outcome\"] == \"LOSS\"].groupby([\"merchant_id\", \"month\"], as_index=False).agg(\n",
        "    loss_amount=(\"dispute_amount_usd\", \"sum\"),\n",
        "    loss_fees=(\"cb_fee_usd\", \"sum\")\n",
        ")\n",
        "\n",
        "mm = txn_mm.merge(disp_mm[[\"merchant_id\", \"month\", \"dispute_count\", \"dispute_amount\"]], on=[\"merchant_id\", \"month\"], how=\"left\")\n",
        "mm = mm.merge(loss_mm, on=[\"merchant_id\", \"month\"], how=\"left\")\n",
        "mm[[\"dispute_count\", \"dispute_amount\", \"loss_amount\", \"loss_fees\"]] = mm[[\"dispute_count\", \"dispute_amount\", \"loss_amount\", \"loss_fees\"]].fillna(0)\n",
        "\n",
        "mm = mm.merge(merchants, on=\"merchant_id\", how=\"left\")\n",
        "mm[\"chargeback_rate\"] = mm[\"dispute_count\"] / mm[\"txn_count\"]\n",
        "mm[\"net_loss_usd\"] = mm[\"loss_amount\"] + mm[\"loss_fees\"]\n",
        "mm[\"net_loss_rate\"] = np.where(mm[\"txn_amount\"] > 0, mm[\"net_loss_usd\"] / mm[\"txn_amount\"], 0)\n",
        "\n",
        "mm.to_csv(os.path.join(OUT_DIR, \"merchant_month_risk_metrics.csv\"), index=False)\n",
        "\n",
        "# 3.2 Dispute reason mapping (overall + by tier)\n",
        "reason_overall = disputes.groupby(\"reason_code\", as_index=False).agg(\n",
        "    disputes=(\"dispute_id\", \"count\"),\n",
        "    dispute_amount=(\"dispute_amount_usd\", \"sum\"),\n",
        "    win_rate=(\"outcome\", lambda x: (x == \"WIN\").mean())\n",
        ").sort_values(\"disputes\", ascending=False)\n",
        "\n",
        "reason_tier = disputes.groupby([\"risk_tier\", \"reason_code\"], as_index=False).agg(\n",
        "    disputes=(\"dispute_id\", \"count\"),\n",
        "    dispute_amount=(\"dispute_amount_usd\", \"sum\"),\n",
        "    win_rate=(\"outcome\", lambda x: (x == \"WIN\").mean())\n",
        ").sort_values([\"risk_tier\", \"disputes\"], ascending=[True, False])\n",
        "\n",
        "reason_overall.to_csv(os.path.join(OUT_DIR, \"dispute_reason_overall.csv\"), index=False)\n",
        "reason_tier.to_csv(os.path.join(OUT_DIR, \"dispute_reason_by_tier.csv\"), index=False)\n",
        "\n",
        "# 3.3 Win/Loss drivers (simple explainable slices)\n",
        "# Evidence quality buckets + response time buckets\n",
        "disputes[\"evidence_bucket\"] = pd.cut(disputes[\"evidence_quality\"], bins=[-1, 40, 60, 80, 100],\n",
        "                                    labels=[\"0-40\", \"41-60\", \"61-80\", \"81-100\"])\n",
        "disputes[\"response_bucket\"] = pd.cut(disputes[\"response_time_days\"], bins=[-1, 2, 7, 14, 30],\n",
        "                                     labels=[\"0-2d\", \"3-7d\", \"8-14d\", \"15-30d\"])\n",
        "\n",
        "drivers = disputes.groupby(\n",
        "    [\"reason_code\", \"evidence_bucket\", \"response_bucket\"],\n",
        "    as_index=False,\n",
        "    observed=True\n",
        ").agg(\n",
        "\n",
        "    disputes=(\"dispute_id\", \"count\"),\n",
        "    avg_amt=(\"dispute_amount_usd\", \"mean\"),\n",
        "    win_rate=(\"outcome\", lambda x: (x == \"WIN\").mean())\n",
        ").sort_values(\"disputes\", ascending=False)\n",
        "\n",
        "drivers.to_csv(os.path.join(OUT_DIR, \"win_loss_drivers_slices.csv\"), index=False)\n",
        "\n",
        "# 3.4 Cohort analysis (merchant tenure cohorts + risk tier)\n",
        "# Create tenure cohorts\n",
        "def tenure_cohort(m):\n",
        "    if m < 6:\n",
        "        return \"0-5m\"\n",
        "    if m < 12:\n",
        "        return \"6-11m\"\n",
        "    if m < 24:\n",
        "        return \"12-23m\"\n",
        "    if m < 36:\n",
        "        return \"24-35m\"\n",
        "    return \"36m+\"\n",
        "\n",
        "mm[\"tenure_cohort\"] = mm[\"tenure_months\"].apply(tenure_cohort)\n",
        "\n",
        "cohort = mm.groupby([\"risk_tier\", \"tenure_cohort\"], as_index=False).agg(\n",
        "    merchants=(\"merchant_id\", \"nunique\"),\n",
        "    txn_count=(\"txn_count\", \"sum\"),\n",
        "    disputes=(\"dispute_count\", \"sum\"),\n",
        "    txn_amount=(\"txn_amount\", \"sum\"),\n",
        "    net_loss_usd=(\"net_loss_usd\", \"sum\")\n",
        ")\n",
        "\n",
        "cohort[\"chargeback_rate\"] = cohort[\"disputes\"] / cohort[\"txn_count\"]\n",
        "cohort[\"net_loss_rate\"] = np.where(cohort[\"txn_amount\"] > 0, cohort[\"net_loss_usd\"] / cohort[\"txn_amount\"], 0)\n",
        "\n",
        "cohort.to_csv(os.path.join(OUT_DIR, \"cohort_risk_summary.csv\"), index=False)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Charts\n",
        "# ---------------------------\n",
        "\n",
        "# 4.1 Overall monthly chargeback rate trend\n",
        "overall_txn = transactions.groupby(\"month\", as_index=False).agg(txn_count=(\"txn_id\", \"count\"))\n",
        "overall_disp = disputes.groupby(\"month\", as_index=False).agg(disputes=(\"dispute_id\", \"count\"))\n",
        "overall = overall_txn.merge(overall_disp, on=\"month\", how=\"left\").fillna(0)\n",
        "overall[\"chargeback_rate\"] = overall[\"disputes\"] / overall[\"txn_count\"]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(pd.to_datetime(overall[\"month\"] + \"-01\"), overall[\"chargeback_rate\"])\n",
        "plt.title(\"Overall Chargeback Rate Trend (Monthly)\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Chargeback Rate\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"overall_chargeback_rate_trend.png\"), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# 4.2 Reason code volume bar\n",
        "plt.figure()\n",
        "plt.bar(reason_overall[\"reason_code\"], reason_overall[\"disputes\"])\n",
        "plt.title(\"Dispute Volume by Reason Code\")\n",
        "plt.xlabel(\"Reason Code\")\n",
        "plt.ylabel(\"Disputes\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"dispute_volume_by_reason.png\"), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# 4.3 Win rate by evidence bucket\n",
        "win_ev = disputes.groupby(\"evidence_bucket\", as_index=False).agg(win_rate=(\"outcome\", lambda x: (x == \"WIN\").mean()),\n",
        "                                                                disputes=(\"dispute_id\", \"count\"))\n",
        "plt.figure()\n",
        "plt.plot(win_ev[\"evidence_bucket\"].astype(str), win_ev[\"win_rate\"], marker=\"o\")\n",
        "plt.title(\"Win Rate by Evidence Quality Bucket\")\n",
        "plt.xlabel(\"Evidence Quality Bucket\")\n",
        "plt.ylabel(\"Win Rate\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"win_rate_by_evidence_bucket.png\"), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# 4.4 Heatmap-ish table (cohort chargeback rate)\n",
        "pivot_cb = cohort.pivot(index=\"tenure_cohort\", columns=\"risk_tier\", values=\"chargeback_rate\")\n",
        "pivot_cb.to_csv(os.path.join(OUT_DIR, \"cohort_chargeback_rate_pivot.csv\"))\n",
        "\n",
        "# Simple table render as image (no seaborn)\n",
        "plt.figure()\n",
        "plt.axis(\"off\")\n",
        "tbl = plt.table(cellText=np.round(pivot_cb.fillna(0).values, 4),\n",
        "                rowLabels=pivot_cb.index.tolist(),\n",
        "                colLabels=pivot_cb.columns.tolist(),\n",
        "                loc=\"center\")\n",
        "tbl.auto_set_font_size(False)\n",
        "tbl.set_fontsize(9)\n",
        "tbl.scale(1, 1.4)\n",
        "plt.title(\"Chargeback Rate by Tenure Cohort x Risk Tier\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"cohort_chargeback_rate_table.png\"), dpi=200)\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nSaved outputs:\")\n",
        "print(\" - outputs/merchant_month_risk_metrics.csv\")\n",
        "print(\" - outputs/dispute_reason_overall.csv\")\n",
        "print(\" - outputs/dispute_reason_by_tier.csv\")\n",
        "print(\" - outputs/win_loss_drivers_slices.csv\")\n",
        "print(\" - outputs/cohort_risk_summary.csv\")\n",
        "print(\" - outputs/overall_chargeback_rate_trend.png\")\n",
        "print(\" - outputs/dispute_volume_by_reason.png\")\n",
        "print(\" - outputs/win_rate_by_evidence_bucket.png\")\n",
        "print(\" - outputs/cohort_chargeback_rate_table.png\")\n",
        "\n",
        "# Quick sanity snapshots\n",
        "print(\"\\nTop 10 reason codes:\")\n",
        "print(reason_overall.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\nCohort summary (sample):\")\n",
        "print(cohort.head(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5mF7fpHAbHu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}